\section{Transfer Learning}

Transfer learning jest zbiorczą nazwą na zestaw technik służących do ponownego użycia i zmiany wyuczonego tematu do rozpoznawania nowego w celu przyspieszenia procesu trenowania sieci neuronowej. Ponieważ tworzenie i uczenie sieci neuronowej wymaga zaangażowania znacznych mocy obliczeniowych, a wykorzystanie transfer learning znacznie obniża to wymaganie jest to technika bardzo popularna i chętnie wykorzystywana w procesie uczenia maszynowego. Aby przyspieszyć i ułatwić ten aspekt pracy z sieciami neuronowymi stworzono zestawy narzędzi ułatwiających to zadanie. Jako przykład posłużyć może dostępna na licencji Apache 2.0 biblioteka Xfer do pobrania z repozytorium Github\cite{xfer_git}.\\

Sieci neuronowe posiadają właściwość nauki korelacji pomiędzy sygnałem wejściowym (np.: obrazem) a jego reprezentacją (np.: opisem). Proces takiej nauki nazywany jest uczeniem nadzorowanym. Po wytrenowaniu sieć jest zdolna do przewidywania powiązania pomiędzy sygnałem wejściowym i najbardziej pasującą reprezentacją. Niestety, jeśli warunki brzegowe w czasie implementacji mogą ulec zmianie lub sieć ma zostać wykorzystana do wykrywania innego rodzaju danych. Np.: sieć wytrenowana do rozpoznawania samolotów w grze komputerowej --- zaaplikowana w aplikacji identyfikującej samoloty z kamery operującej na prawdziwym lotnisku. Sieć może nie znać części samolotów, nie będzie ich więc w stanie rozpoznać, w świecie rzeczywistym na jakość sygnału wejściowego będą miały wpływ warunki atmosferyczne itp..
Aby zapewnić wyższą skuteczność rozpoznawania sieć powinna zostać wytrenowana na danych, na których ma pracować. Bardzo często nie jest to możliwe, bądź nie jest opłacalne ze względu na wymagany czas, nakłady mocy obliczeniowej... Możliwy i częsty jest scenariusz, w którym nie dysponujemy odpowiednio duża ilością przykładów do nauki sieci. W omawianym przykładnie może to być np.: przygotowanie się do rozpoznawania samolotów w warunkach ciężkiej zimy posiadając ograniczoną ilość zdjęć samolotów w czasie śnieżycy. W takich właśnie przypadkach wykorzystanie transfer learning jest najbardziej wskazane. Pomimo, że oryginalnie rozpoznawane obiekty i nowe są różne, posiadają one zawsze jakieś cechy wspólne (powinny takie posiadać). W takim przypadku sieć może kontynuować przerwaną naukę, bazując na elementach wspólnych wykrywanych obiektów.\\

Podsumowując słowami Andei'a Karpathy\cite{karpathy:CS231}: praktycznie niewiele osób trenuje całą sieć konwulsyjną (Convutional Network) od początku (z losową inicjlizacją wag sieci) ze względu na brak zestawu danych odpowiedniej wielkości. Zamiast tego wykorzystywane są wytrenowane na dużych zestawach danych modele, które de-facto inicjalizują wagi sieci docelowej.




